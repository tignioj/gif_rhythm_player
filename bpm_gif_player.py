import cv2
import time
import numpy as np
import os
import threading
from collections import deque
import sounddevice as sd
import essentia.standard as es

# ---------- å…¨å±€å…±äº«å˜é‡ ----------
current_bpm = 120.0          # é»˜è®¤ BPM
bpm_lock = threading.Lock()  # ä¿æŠ¤ current_bpm
running = True              # æ§åˆ¶çº¿ç¨‹é€€å‡º

# ---------- BPM æ£€æµ‹å‚æ•° ----------
SAMPLE_RATE = 22050
# SAMPLE_RATE = 48000
WINDOW_SECONDS = 8
BLOCK_SIZE = 1024
CHANNELS = 1

audio_buffer = deque()
buffer_lock = threading.Lock()

# ---------- éŸ³é¢‘å›è°ƒ ----------
def audio_callback(indata, frames, time_info, status):
    if status:
        print(f"âš ï¸ çŠ¶æ€ï¼š{status}")
    with buffer_lock:
        audio = indata.copy().astype(np.float32)
        if audio.shape[1] > 1:
            audio = np.mean(audio, axis=1, keepdims=True)
        audio_buffer.append((time.time(), audio.flatten()))

# ---------- Essentia BPM ä¼°ç®— ----------
def estimate_bpm(audio_signal, sample_rate=SAMPLE_RATE):
    bpm_estimator = es.PercivalBpmEstimator(sampleRate=sample_rate)
    try:
        return bpm_estimator(audio_signal)
    except Exception as e:
        print(f"âŒ BPM ä¼°ç®—å¤±è´¥ï¼š{e}")
        return 0.0

# ---------- è·å–æœ€è¿‘çª—å£éŸ³é¢‘ ----------
def get_recent_audio():
    with buffer_lock:
        if not audio_buffer:
            return None
        now = time.time()
        cutoff = now - WINDOW_SECONDS
        chunks = [block for ts, block in audio_buffer if ts >= cutoff]
        if not chunks:
            return None
        full_audio = np.concatenate(chunks)
        return full_audio

# ---------- BPM æ£€æµ‹çº¿ç¨‹å‡½æ•° ----------
def bpm_detection_loop():
    global current_bpm
    print("ğŸ¤ å¯åŠ¨ BPM æ£€æµ‹çº¿ç¨‹...")
    stream = sd.InputStream(
        samplerate=SAMPLE_RATE,
        blocksize=BLOCK_SIZE,
        channels=CHANNELS,
        callback=audio_callback,
        dtype='float32'
    )
    with stream:
        while running:
            time.sleep(WINDOW_SECONDS)
            audio = get_recent_audio()
            if audio is None or len(audio) < SAMPLE_RATE * 0.5:
                print("â³ ç­‰å¾…è¶³å¤ŸéŸ³é¢‘æ•°æ®...")
                continue
            audio = np.clip(audio, -1.0, 1.0)
            bpm = estimate_bpm(audio, SAMPLE_RATE)
            if bpm > 0:
                with bpm_lock:
                    current_bpm = bpm
                print(f"ğŸµ æ›´æ–° BPM ä¸ºï¼š{bpm:.1f}")
            else:
                print("âš ï¸ æ— æ³•æ£€æµ‹ç¨³å®šèŠ‚æ‹")

# ---------- æ’­æ”¾ GIF å‡½æ•°ï¼ˆå®æ—¶è¯»å–å…¨å±€ BPMï¼‰----------
def play_gif_with_beat_pattern(
    gif_path,
    beat_pattern=(1, 0, 1, 0),
    frames_per_beat=6,
):
    global running
    cap = cv2.VideoCapture(gif_path)
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€ GIF")
        return

    gif_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if gif_frame_count <= 0:
        print("âŒ æ— æ•ˆçš„ GIF å¸§æ•°")
        return

    pattern_len = len(beat_pattern)
    start_time = time.perf_counter()

    print(f"â–¶ï¸ å¼€å§‹æ’­æ”¾ GIFï¼Œåˆå§‹ BPM: {current_bpm:.1f}")
    print(f"æ‹å­æ¨¡å¼: {beat_pattern}")
    print(f"æ¯æ‹å¸§æ•°: {frames_per_beat}")

    while running:
        # 1. è·å–å½“å‰æœ€æ–°çš„ BPM
        with bpm_lock:
            bpm = current_bpm

        beat_interval = 60.0 / bpm

        # 2. æ—¶é—´è®¡ç®—
        now = time.perf_counter()
        elapsed = now - start_time

        beat_index_global = int(elapsed / beat_interval)
        beat_index = beat_index_global % pattern_len
        is_strong = beat_pattern[beat_index] == 1

        beat_phase = (elapsed % beat_interval) / beat_interval
        frame_in_beat = int(beat_phase * frames_per_beat)
        frame_in_beat = min(frame_in_beat, frames_per_beat - 1)

        gif_frame_index = (
            beat_index_global * frames_per_beat + frame_in_beat
        ) % gif_frame_count

        # 3. è¯»å–å¹¶æ˜¾ç¤ºå¸§
        cap.set(cv2.CAP_PROP_POS_FRAMES, gif_frame_index)
        # ret, frame = cap.read()
        ret, frame = cap.read()
        if not ret:
            continue
        frame = cv2.resize(frame, (240,240))

        # æ˜¾ç¤ºå½“å‰ BPM å’Œæ‹ä¿¡æ¯
        # cv2.putText(frame, f"BPM: {bpm:.1f}", (40, 30),
        #             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        # cv2.putText(frame, f"Beat frame: {frame_in_beat+1}/{frames_per_beat}",
        #             (40, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)
        # cv2.putText(frame, f"Pattern idx: {beat_index}",
        #             (40, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200,200,200), 2)

        cv2.imshow("GIF Beat Sync (Live BPM)", frame)

        # 4. æŒ‰ ESC é€€å‡º
        if cv2.waitKey(1) == 27:
            running = False
            break

    cap.release()
    cv2.destroyAllWindows()

# ---------- ä¸»ç¨‹åº ----------
if __name__ == "__main__":
    # gif_path = "resources/gif/dance4.gif"
    gif_path = "resources/gif/common.gif"
    # gif_path = os.path.join(application_path, "resources/gif/common.gif")

    # å¯åŠ¨ BPM æ£€æµ‹çº¿ç¨‹ï¼ˆåå°ï¼‰
    bpm_thread = threading.Thread(target=bpm_detection_loop, daemon=True)
    bpm_thread.start()

    # ä¸»çº¿ç¨‹æ’­æ”¾ GIFï¼ˆé˜»å¡ï¼‰
    play_gif_with_beat_pattern(
        gif_path,
        beat_pattern=(1, 0, 1, 0),
        frames_per_beat=13,
    )